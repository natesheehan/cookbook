{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Computational Methods for Historical Texts is an interdisciplinary guide that marries the fields of computer science and historical studies, offering insights and methodologies for analyzing historical documents through computational means. This book is aimed at a diverse audience that includes historians, linguists, computer scientists, and scholars from various disciplines, emphasizing the importance of a multidisciplinary approach in the digital humanities. It introduces foundational concepts and practical techniques for employing text mining, natural language processing, and other computational tools to facilitate the study of historical texts. People Dr Tristan Cann : computational social scientist, Department of Computer Science, University of Exeter Nathanael Sheehan : PhD Student, Department of Computer Science, University of Exeter Dr Barbara Caddick : primary health care researcher; social and cultural historian, Centre for Academic Primary Care and honorary Department of History, University of Bristol. Dr Joanna Clifton-Sprigg: labour economist, Department of Economics, University of Bath Dr C\u00e9sar Jim\u00e9nez-Mart\u00ednez : media and cultural visibility researcher, Department of Media and Communications, LSE Dr Thomas Larkin : Sino-American relations researcher, Department of History and Classics, University of Prince Edward Island","title":"About"},{"location":"#about","text":"Computational Methods for Historical Texts is an interdisciplinary guide that marries the fields of computer science and historical studies, offering insights and methodologies for analyzing historical documents through computational means. This book is aimed at a diverse audience that includes historians, linguists, computer scientists, and scholars from various disciplines, emphasizing the importance of a multidisciplinary approach in the digital humanities. It introduces foundational concepts and practical techniques for employing text mining, natural language processing, and other computational tools to facilitate the study of historical texts.","title":"About"},{"location":"#people","text":"Dr Tristan Cann : computational social scientist, Department of Computer Science, University of Exeter Nathanael Sheehan : PhD Student, Department of Computer Science, University of Exeter Dr Barbara Caddick : primary health care researcher; social and cultural historian, Centre for Academic Primary Care and honorary Department of History, University of Bristol. Dr Joanna Clifton-Sprigg: labour economist, Department of Economics, University of Bath Dr C\u00e9sar Jim\u00e9nez-Mart\u00ednez : media and cultural visibility researcher, Department of Media and Communications, LSE Dr Thomas Larkin : Sino-American relations researcher, Department of History and Classics, University of Prince Edward Island","title":"People"},{"location":"%281%29%20intro/","text":"","title":"(1) intro"},{"location":"%282%29%20data/","text":"","title":"(2) data"},{"location":"%283%29%20simple%20analysis/","text":"Simple Analysis Installing Package pip install spacy python -m spacy download en_core_web_sm Tokenization and Sentence Segmentation The division of text into meaningful units such as words, phrases, and sentences is foundational in NLP. SpaCy excels in breaking down text into tokens and sentences, providing a structured format for analysis. import spacy # Load SpaCy's English language model nlp = spacy.load('en_core_web_sm') # Process a sample text text = \"In the late fight of the 20th century, the world witnessed a significant shift in the fight against HIV/AIDS.\" doc = nlp(text) # Tokenization tokens = [token.text for token in doc] print(\"Tokens:\", tokens) # Sentence Segmentation sentences = [sent.text for sent in doc.sents] print(\"Sentences:\", sentences) This snippet outputs each word and punctuation mark as separate tokens, showcasing SpaCy's precision in distinguishing textual elements. sentences = list(doc.sents) print(\"Sentence 1:\", sentences[0].text)","title":"Simple Analysis"},{"location":"%283%29%20simple%20analysis/#simple-analysis","text":"","title":"Simple Analysis"},{"location":"%283%29%20simple%20analysis/#installing-package","text":"pip install spacy python -m spacy download en_core_web_sm","title":"Installing Package"},{"location":"%283%29%20simple%20analysis/#tokenization-and-sentence-segmentation","text":"The division of text into meaningful units such as words, phrases, and sentences is foundational in NLP. SpaCy excels in breaking down text into tokens and sentences, providing a structured format for analysis. import spacy # Load SpaCy's English language model nlp = spacy.load('en_core_web_sm') # Process a sample text text = \"In the late fight of the 20th century, the world witnessed a significant shift in the fight against HIV/AIDS.\" doc = nlp(text) # Tokenization tokens = [token.text for token in doc] print(\"Tokens:\", tokens) # Sentence Segmentation sentences = [sent.text for sent in doc.sents] print(\"Sentences:\", sentences) This snippet outputs each word and punctuation mark as separate tokens, showcasing SpaCy's precision in distinguishing textual elements. sentences = list(doc.sents) print(\"Sentence 1:\", sentences[0].text)","title":"Tokenization and Sentence Segmentation"},{"location":"%284%29%20basic%20plotting/","text":"","title":"(4) basic plotting"},{"location":"%286%29%20pre-trained%20models/","text":"","title":"(6) pre trained models"},{"location":"%287%29%20text%20embedding/","text":"","title":"(7) text embedding"},{"location":"faq/","text":"","title":"Faq"}]}