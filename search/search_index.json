{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"About Computational Methods for Historical Texts is an interdisciplinary guide that marries the fields of computer science and historical studies, offering insights and methodologies for analyzing historical documents through computational means. This book is aimed at a diverse audience that includes historians, linguists, computer scientists, and scholars from various disciplines, emphasizing the importance of a multidisciplinary approach in the digital humanities. It introduces foundational concepts and practical techniques for employing text mining, natural language processing, and other computational tools to facilitate the study of historical texts. People Dr Tristan Cann : computational social scientist, Department of Computer Science, University of Exeter Nathanael Sheehan : PhD Student, Department of Computer Science, University of Exeter Dr Barbara Caddick : primary health care researcher; social and cultural historian, Centre for Academic Primary Care and honorary Department of History, University of Bristol. Dr Joanna Clifton-Sprigg: labour economist, Department of Economics, University of Bath Dr C\u00e9sar Jim\u00e9nez-Mart\u00ednez : media and cultural visibility researcher, Department of Media and Communications, LSE Dr Thomas Larkin : Sino-American relations researcher, Department of History and Classics, University of Prince Edward Island","title":"About"},{"location":"#about","text":"Computational Methods for Historical Texts is an interdisciplinary guide that marries the fields of computer science and historical studies, offering insights and methodologies for analyzing historical documents through computational means. This book is aimed at a diverse audience that includes historians, linguists, computer scientists, and scholars from various disciplines, emphasizing the importance of a multidisciplinary approach in the digital humanities. It introduces foundational concepts and practical techniques for employing text mining, natural language processing, and other computational tools to facilitate the study of historical texts.","title":"About"},{"location":"#people","text":"Dr Tristan Cann : computational social scientist, Department of Computer Science, University of Exeter Nathanael Sheehan : PhD Student, Department of Computer Science, University of Exeter Dr Barbara Caddick : primary health care researcher; social and cultural historian, Centre for Academic Primary Care and honorary Department of History, University of Bristol. Dr Joanna Clifton-Sprigg: labour economist, Department of Economics, University of Bath Dr C\u00e9sar Jim\u00e9nez-Mart\u00ednez : media and cultural visibility researcher, Department of Media and Communications, LSE Dr Thomas Larkin : Sino-American relations researcher, Department of History and Classics, University of Prince Edward Island","title":"People"},{"location":"%281%29%20intro/","text":"","title":"(1) intro"},{"location":"%282%29%20data/","text":"Data collecton and loading Sourcing data A wide range of online sources are available for collecting data on a similar number of topics. Online archives and repositories often make raw text files available, or text can be extracted from other available content such as web pages or PDFs. We won't cover the details of collecting a dataset here, instead assuming that you have already collected your own set of text files to work on. Python can handle a range of different file formats as input, but some of the steps will need to be customised depending on your chosen format. For demonstrative purposes, this cookbook uses a set of AI-generated content on a range of themes (primarily to respect the copyright of works we use in research). You can see more information how these data were generated in the data folder . If you are interested in interacting with online sources of data, you may find the following examples helpful: - Introduction to web scraping - Introduction to accessing APIs Working with external data Once you've collected a dataset, you are ready to begin working with the text in Python. There are a wide range of file handling options that can automatically parse different structures. We'll give you some common examples below. Loading data A .text file is the simplest format for handling text data. Here is a snippet of code to read one in as a single string. with open('example.txt','r',encoding='utf-8') as f: text = '\\n'.join(f) Note the encoding parameter we used here, which is useful for handling a range of special characters and is typically required for processing web data. Alternatively, you may have created a .csv file containing information for a range of different documents. Pandas is a useful library for automatically converting these data into a convenient format. import pandas as pd df = pd.read_csv('example.csv') We're going to use a slightly different format for working with our example text generated using AI tools called a .json file. This format is a good way to attach relevant metadata to a document text. import json with open('example.json','r',encoding='utf-8') as f: data = json.load(f) Making a corpus The above examples all work on a single file, but can be easily expanded to cover an entire corpus of documents. Here's an example that works for all files in a folder. import os files = os.listdir('example_folder') corpus = [] for file in files: with open(f'example_folder/{file}','r',encoding='utf-8') as f: corpus.append(json.load(f)) A note about pre-processing One difference between the examples we're presenting here and many practical applications of natural language processing is the need for pre-processing of data. Our examples are carefully constructed to require minimal pre-processing to correct OCR or transcription mistake or remove unncessary content. Some example of basic examples are included on the next page, or you can read more detail via an online tutorial","title":"Data collecton and loading"},{"location":"%282%29%20data/#data-collecton-and-loading","text":"","title":"Data collecton and loading"},{"location":"%282%29%20data/#sourcing-data","text":"A wide range of online sources are available for collecting data on a similar number of topics. Online archives and repositories often make raw text files available, or text can be extracted from other available content such as web pages or PDFs. We won't cover the details of collecting a dataset here, instead assuming that you have already collected your own set of text files to work on. Python can handle a range of different file formats as input, but some of the steps will need to be customised depending on your chosen format. For demonstrative purposes, this cookbook uses a set of AI-generated content on a range of themes (primarily to respect the copyright of works we use in research). You can see more information how these data were generated in the data folder . If you are interested in interacting with online sources of data, you may find the following examples helpful: - Introduction to web scraping - Introduction to accessing APIs","title":"Sourcing data"},{"location":"%282%29%20data/#working-with-external-data","text":"Once you've collected a dataset, you are ready to begin working with the text in Python. There are a wide range of file handling options that can automatically parse different structures. We'll give you some common examples below.","title":"Working with external data"},{"location":"%282%29%20data/#loading-data","text":"A .text file is the simplest format for handling text data. Here is a snippet of code to read one in as a single string. with open('example.txt','r',encoding='utf-8') as f: text = '\\n'.join(f) Note the encoding parameter we used here, which is useful for handling a range of special characters and is typically required for processing web data. Alternatively, you may have created a .csv file containing information for a range of different documents. Pandas is a useful library for automatically converting these data into a convenient format. import pandas as pd df = pd.read_csv('example.csv') We're going to use a slightly different format for working with our example text generated using AI tools called a .json file. This format is a good way to attach relevant metadata to a document text. import json with open('example.json','r',encoding='utf-8') as f: data = json.load(f)","title":"Loading data"},{"location":"%282%29%20data/#making-a-corpus","text":"The above examples all work on a single file, but can be easily expanded to cover an entire corpus of documents. Here's an example that works for all files in a folder. import os files = os.listdir('example_folder') corpus = [] for file in files: with open(f'example_folder/{file}','r',encoding='utf-8') as f: corpus.append(json.load(f))","title":"Making a corpus"},{"location":"%282%29%20data/#a-note-about-pre-processing","text":"One difference between the examples we're presenting here and many practical applications of natural language processing is the need for pre-processing of data. Our examples are carefully constructed to require minimal pre-processing to correct OCR or transcription mistake or remove unncessary content. Some example of basic examples are included on the next page, or you can read more detail via an online tutorial","title":"A note about pre-processing"},{"location":"%283%29%20simple%20analysis/","text":"Simple Analysis Installing Package pip install spacy python -m spacy download en_core_web_sm Tokenization and Sentence Segmentation The division of text into meaningful units such as words, phrases, and sentences is foundational in NLP. SpaCy excels in breaking down text into tokens and sentences, providing a structured format for analysis. import spacy # Load SpaCy's English language model nlp = spacy.load('en_core_web_sm') # Process a sample text text = \"In the late fight of the 20th century, the world witnessed a significant shift in the fight against HIV/AIDS.\" doc = nlp(text) # Tokenization tokens = [token.text for token in doc] print(\"Tokens:\", tokens) # Sentence Segmentation sentences = [sent.text for sent in doc.sents] print(\"Sentences:\", sentences) This snippet outputs each word and punctuation mark as separate tokens, showcasing SpaCy's precision in distinguishing textual elements. sentences = list(doc.sents) print(\"Sentence 1:\", sentences[0].text)","title":"Simple Analysis"},{"location":"%283%29%20simple%20analysis/#simple-analysis","text":"","title":"Simple Analysis"},{"location":"%283%29%20simple%20analysis/#installing-package","text":"pip install spacy python -m spacy download en_core_web_sm","title":"Installing Package"},{"location":"%283%29%20simple%20analysis/#tokenization-and-sentence-segmentation","text":"The division of text into meaningful units such as words, phrases, and sentences is foundational in NLP. SpaCy excels in breaking down text into tokens and sentences, providing a structured format for analysis. import spacy # Load SpaCy's English language model nlp = spacy.load('en_core_web_sm') # Process a sample text text = \"In the late fight of the 20th century, the world witnessed a significant shift in the fight against HIV/AIDS.\" doc = nlp(text) # Tokenization tokens = [token.text for token in doc] print(\"Tokens:\", tokens) # Sentence Segmentation sentences = [sent.text for sent in doc.sents] print(\"Sentences:\", sentences) This snippet outputs each word and punctuation mark as separate tokens, showcasing SpaCy's precision in distinguishing textual elements. sentences = list(doc.sents) print(\"Sentence 1:\", sentences[0].text)","title":"Tokenization and Sentence Segmentation"},{"location":"%284%29%20basic%20plotting/","text":"","title":"(4) basic plotting"},{"location":"%286%29%20pre-trained%20models/","text":"","title":"(6) pre trained models"},{"location":"%287%29%20text%20embedding/","text":"","title":"(7) text embedding"},{"location":"faq/","text":"","title":"Faq"}]}